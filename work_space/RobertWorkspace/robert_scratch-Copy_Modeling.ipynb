{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import timedelta, datetime\n",
    "import datetime\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.preprocessing\n",
    "import explore\n",
    "import prepare\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('accident_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare import clean_collision_data\n",
    "df = clean_collision_data()\n",
    "#cross validation on train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = prepare.collision_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21336, 47), (5334, 47))"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crash_date', 'crash_day', 'crash_hour', 'crash_id', 'crash_latitude',\n",
       "       'crash_longitude', 'crash_occupant_count', 'crash_vehicle_count',\n",
       "       'damage_air', 'damage_airbag', 'damage_burned', 'damage_concentrated',\n",
       "       'damage_distributed', 'damage_rollover', 'damage_zone', 'dl_cdl',\n",
       "       'dl_class_a', 'dl_class_b', 'dl_class_m', 'dl_state', 'dl_unlicensed',\n",
       "       'driver_age', 'driver_age_bin', 'driver_male', 'driver_race',\n",
       "       'factors_road', 'factors_spd_lmt_mph', 'factors_weather', 'fault_class',\n",
       "       'fault_distraction', 'fault_fatigue', 'fault_intoxication',\n",
       "       'fault_maneuver', 'fault_narrative', 'fault_speed', 'fault_yield',\n",
       "       'injury_class', 'injury_crash_total', 'speed_speed_lm',\n",
       "       'speed_yield_occu', 'vehicle_color', 'vehicle_id', 'vehicle_make',\n",
       "       'vehicle_occupant_count', 'vehicle_type', 'vehicle_year',\n",
       "       'vehicle_year_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17852\n",
       "1     3484\n",
       "Name: injury_class, dtype: int64"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.injury_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21336, 47)"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21336, 32), (21336,))"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the dataset into train features and target\n",
    "X_train = train.select_dtypes(np.number).drop(columns = ['injury_class', 'injury_crash_total'])\n",
    "y_train = train.injury_class\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5334, 32), (5334,))"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the dataset into test features and target\n",
    "X_test = test.select_dtypes(np.number).drop(columns = ['injury_class', 'injury_crash_total'])\n",
    "y_test = test.injury_class\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizing dummy classifier to create baseline\n",
    "dummy = DummyClassifier(strategy ='most_frequent')\n",
    "#fitting on X_train, y_train\n",
    "dummy.fit(X_train, y_train)\n",
    "#creating the baseline\n",
    "baseline = pd.Series(dummy.predict(X_train), index = X_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the baseline prediciton to no injury\n",
    "train['baseline_prediction'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8367079115110612"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy calculation\n",
    "baseline_accuracy = (train.baseline_prediction == y_train).mean()\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizing smote \n",
    "smote = SMOTE(random_state = 19)\n",
    "# fitting on train\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to scale the data\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the scalar to train\n",
    "scaler.fit(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to scale the X test dataset\n",
    "X_smote = pd.DataFrame(scaler.transform(X_smote), columns = X_smote.columns, index = X_smote.index)\n",
    "#scaling the test dataset\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns, index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizing smote \n",
    "smote = SMOTE(random_state = 19)\n",
    "# fitting on train\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to scale the data\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the scalar to train\n",
    "scaler.fit(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to scale the X test dataset\n",
    "X_smote = pd.DataFrame(scaler.transform(X_smote), columns = X_smote.columns, index = X_smote.index)\n",
    "#scaling the test dataset\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns, index = X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the object\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_smote' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-459-4692dcded47d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fitting our thing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_smote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_smote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#creating the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_smote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#creating prediction probabaility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_smote' is not defined"
     ]
    }
   ],
   "source": [
    "# fitting our thing\n",
    "clf = clf.fit(X_smote, y_smote)\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_smote)\n",
    "#creating prediction probabaility\n",
    "y_pred_proba = clf.predict_proba(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.73\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_smote, y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12292</td>\n",
       "      <td>5560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4109</td>\n",
       "      <td>13743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  12292   5560\n",
       "1   4109  13743"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_smote.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_smote, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.72     17852\n",
      "           1       0.71      0.77      0.74     17852\n",
      "\n",
      "    accuracy                           0.73     35704\n",
      "   macro avg       0.73      0.73      0.73     35704\n",
      "weighted avg       0.73      0.73      0.73     35704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our thing\n",
    "rf = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight= 'balanced', \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=6,\n",
    "                            n_estimators=150,\n",
    "                            max_depth=12, \n",
    "                            random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=12,\n",
       "                       min_samples_leaf=6, n_estimators=150, random_state=19)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our thing\n",
    "rf.fit(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_smote)\n",
    "y_pred_proba = rf.predict_proba(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on training set: 0.86\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf.score(X_smote, y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.86     17852\n",
      "           1       0.88      0.84      0.86     17852\n",
      "\n",
      "    accuracy                           0.86     35704\n",
      "   macro avg       0.86      0.86      0.86     35704\n",
      "weighted avg       0.86      0.86      0.86     35704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_smote, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.78\n",
      "[[3879  584]\n",
      " [ 588  283]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      4463\n",
      "           1       0.33      0.32      0.33       871\n",
      "\n",
      "    accuracy                           0.78      5334\n",
      "   macro avg       0.60      0.60      0.60      5334\n",
      "weighted avg       0.78      0.78      0.78      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test_scaled)\n",
    "y_pred_proba = rf.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(rf.score(X_test_scaled, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our thing \n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our thing\n",
    "knn.fit(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the prediction\n",
    "y_pred = knn.predict(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estmating the probability\n",
    "y_pred_proba = knn.predict_proba(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.90\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_smote, y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89     17852\n",
      "           1       0.88      0.92      0.90     17852\n",
      "\n",
      "    accuracy                           0.90     35704\n",
      "   macro avg       0.90      0.90      0.90     35704\n",
      "weighted avg       0.90      0.90      0.90     35704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create out thing\n",
    "logit = LogisticRegression(C=1, random_state=19, intercept_scaling=1, solver='lbfgs', class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', random_state=19)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the thing\n",
    "logit.fit(X_smote, y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the predicition\n",
    "y_pred = logit.predict(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate the probability\n",
    "y_pred_proba = logit.predict_proba(X_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.75\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_smote, y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75     17852\n",
      "           1       0.74      0.77      0.76     17852\n",
      "\n",
      "    accuracy                           0.75     35704\n",
      "   macro avg       0.75      0.75      0.75     35704\n",
      "weighted avg       0.75      0.75      0.75     35704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Booster Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the object\n",
    "clf = GradientBoostingClassifier(learning_rate=1.0, max_depth=6, random_state=19).fit(X_resampled, y_resampled)\n",
    "# fitting our thing\n",
    "clf = clf.fit(X_resampled, y_resampled)\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)\n",
    "#creating prediction probabaility\n",
    "y_pred_proba = clf.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3482</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3482     2\n",
       "1     5  3479"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3484\n",
      "           1       1.00      1.00      1.00      3484\n",
      "\n",
      "    accuracy                           1.00      6968\n",
      "   macro avg       1.00      1.00      1.00      6968\n",
      "weighted avg       1.00      1.00      1.00      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "[[2920 1543]\n",
      " [ 302  569]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      4463\n",
      "           1       0.27      0.65      0.38       871\n",
      "\n",
      "    accuracy                           0.65      5334\n",
      "   macro avg       0.59      0.65      0.57      5334\n",
      "weighted avg       0.80      0.65      0.70      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.69\n",
      "[[3321 1142]\n",
      " [ 508  363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.74      0.80      4463\n",
      "           1       0.24      0.42      0.31       871\n",
      "\n",
      "    accuracy                           0.69      5334\n",
      "   macro avg       0.55      0.58      0.55      5334\n",
      "weighted avg       0.77      0.69      0.72      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "y_pred_proba = logit.predict_proba(X_test)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17852\n",
       "1     3484\n",
       "Name: injury_class, dtype: int64"
      ]
     },
     "execution_count": 1039,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.injury_class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=19,)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6968, 32), (6968,))"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape , y_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to scale the data\n",
    "#scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the scalar to train\n",
    "#scaler.fit(X_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Booster Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the object\n",
    "clf = GradientBoostingClassifier(learning_rate= 0.5, max_depth=6, random_state=19).fit(X_resampled, y_resampled)\n",
    "# fitting our thing\n",
    "clf = clf.fit(X_resampled, y_resampled)\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)\n",
    "#creating prediction probabaility\n",
    "y_pred_proba = clf.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3473</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>3454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3473    11\n",
       "1    30  3454"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3484\n",
      "           1       1.00      0.99      0.99      3484\n",
      "\n",
      "    accuracy                           0.99      6968\n",
      "   macro avg       0.99      0.99      0.99      6968\n",
      "weighted avg       0.99      0.99      0.99      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64\n",
      "[[2845 1618]\n",
      " [ 298  573]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.64      0.75      4463\n",
      "           1       0.26      0.66      0.37       871\n",
      "\n",
      "    accuracy                           0.64      5334\n",
      "   macro avg       0.58      0.65      0.56      5334\n",
      "weighted avg       0.80      0.64      0.69      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the object\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=19,criterion = 'entropy')\n",
    "# fitting our thing\n",
    "clf = clf.fit(X_resampled, y_resampled)\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)\n",
    "#creating prediction probabaility\n",
    "y_pred_proba = clf.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.67\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2398</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1216</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  2398  1086\n",
       "1  1216  2268"
      ]
     },
     "execution_count": 1048,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68      3484\n",
      "           1       0.68      0.65      0.66      3484\n",
      "\n",
      "    accuracy                           0.67      6968\n",
      "   macro avg       0.67      0.67      0.67      6968\n",
      "weighted avg       0.67      0.67      0.67      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "[[3066 1397]\n",
      " [ 314  557]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.78      4463\n",
      "           1       0.29      0.64      0.39       871\n",
      "\n",
      "    accuracy                           0.68      5334\n",
      "   macro avg       0.60      0.66      0.59      5334\n",
      "weighted avg       0.81      0.68      0.72      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our thing\n",
    "rf = RandomForestClassifier(bootstrap=True, \n",
    "                         \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=5,\n",
    "                            n_estimators=200,\n",
    "                            max_depth=8, \n",
    "                            random_state=19)\n",
    "\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, min_samples_leaf=5, n_estimators=200,\n",
       "                       random_state=19)"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our thing\n",
    "rf.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.72\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(rf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2398</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1216</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  2398  1086\n",
       "1  1216  2268"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.68      3484\n",
      "           1       0.68      0.65      0.66      3484\n",
      "\n",
      "    accuracy                           0.67      6968\n",
      "   macro avg       0.67      0.67      0.67      6968\n",
      "weighted avg       0.67      0.67      0.67      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "[[3066 1397]\n",
      " [ 314  557]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.78      4463\n",
      "           1       0.29      0.64      0.39       871\n",
      "\n",
      "    accuracy                           0.68      5334\n",
      "   macro avg       0.60      0.66      0.59      5334\n",
      "weighted avg       0.81      0.68      0.72      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.71\n",
      "[[3222 1241]\n",
      " [ 306  565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.81      4463\n",
      "           1       0.31      0.65      0.42       871\n",
      "\n",
      "    accuracy                           0.71      5334\n",
      "   macro avg       0.61      0.69      0.61      5334\n",
      "weighted avg       0.82      0.71      0.74      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "y_pred_proba = rf.predict_proba(X_test)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(rf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our thing \n",
    "knn = KNeighborsClassifier(n_neighbors=10, leaf_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(leaf_size=20, n_neighbors=10)"
      ]
     },
     "execution_count": 1062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our thing\n",
    "knn.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the prediction\n",
    "y_pred = knn.predict(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estmating the probability\n",
    "y_pred_proba = knn.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.69\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.72      3484\n",
      "           1       0.76      0.56      0.64      3484\n",
      "\n",
      "    accuracy                           0.69      6968\n",
      "   macro avg       0.70      0.69      0.68      6968\n",
      "weighted avg       0.70      0.69      0.68      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.57\n",
      "[[2671 1792]\n",
      " [ 496  375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.60      0.70      4463\n",
      "           1       0.17      0.43      0.25       871\n",
      "\n",
      "    accuracy                           0.57      5334\n",
      "   macro avg       0.51      0.51      0.47      5334\n",
      "weighted avg       0.73      0.57      0.63      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "y_pred_proba = knn.predict_proba(X_test)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(knn.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create out thing\n",
    "logit = LogisticRegression(C=1, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, random_state=19)"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the thing\n",
    "logit.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the predicition\n",
    "y_pred = logit.predict(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate the probability\n",
    "y_pred_proba = logit.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.50\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3484    0]\n",
      " [3484    0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      3484\n",
      "           1       0.00      0.00      0.00      3484\n",
      "\n",
      "    accuracy                           0.50      6968\n",
      "   macro avg       0.25      0.50      0.33      6968\n",
      "weighted avg       0.25      0.50      0.33      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "[[4463    0]\n",
      " [ 871    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      4463\n",
      "           1       0.00      0.00      0.00       871\n",
      "\n",
      "    accuracy                           0.84      5334\n",
      "   macro avg       0.42      0.50      0.46      5334\n",
      "weighted avg       0.70      0.84      0.76      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "y_pred_proba = logit.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "nm1 = NearMiss(version=3)\n",
    "X_resampled, y_resampled = nm1.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the object\n",
    "clf = DecisionTreeClassifier(max_depth = 8, random_state=19)\n",
    "# fitting our thing\n",
    "clf = clf.fit(X_resampled, y_resampled)\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)\n",
    "#creating prediction probabaility\n",
    "y_pred_proba = clf.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.73\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2744</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1164</td>\n",
       "      <td>2320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  2744   740\n",
       "1  1164  2320"
      ]
     },
     "execution_count": 1081,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3484\n",
      "           1       0.76      0.67      0.71      3484\n",
      "\n",
      "    accuracy                           0.73      6968\n",
      "   macro avg       0.73      0.73      0.73      6968\n",
      "weighted avg       0.73      0.73      0.73      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "[[3270 1193]\n",
      " [ 375  496]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81      4463\n",
      "           1       0.29      0.57      0.39       871\n",
      "\n",
      "    accuracy                           0.71      5334\n",
      "   macro avg       0.60      0.65      0.60      5334\n",
      "weighted avg       0.80      0.71      0.74      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our thing\n",
    "rf = RandomForestClassifier(bootstrap=True, \n",
    "                         \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=5,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=8, \n",
    "                            random_state=19)\n",
    "\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, min_samples_leaf=5, random_state=19)"
      ]
     },
     "execution_count": 1086,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our thing\n",
    "rf.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.72\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(rf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2744</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1164</td>\n",
       "      <td>2320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  2744   740\n",
       "1  1164  2320"
      ]
     },
     "execution_count": 1088,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      3484\n",
      "           1       0.76      0.67      0.71      3484\n",
      "\n",
      "    accuracy                           0.73      6968\n",
      "   macro avg       0.73      0.73      0.73      6968\n",
      "weighted avg       0.73      0.73      0.73      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "[[3270 1193]\n",
      " [ 375  496]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.73      0.81      4463\n",
      "           1       0.29      0.57      0.39       871\n",
      "\n",
      "    accuracy                           0.71      5334\n",
      "   macro avg       0.60      0.65      0.60      5334\n",
      "weighted avg       0.80      0.71      0.74      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.71\n",
      "[[3207 1256]\n",
      " [ 304  567]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.80      4463\n",
      "           1       0.31      0.65      0.42       871\n",
      "\n",
      "    accuracy                           0.71      5334\n",
      "   macro avg       0.61      0.68      0.61      5334\n",
      "weighted avg       0.82      0.71      0.74      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "y_pred_proba = rf.predict_proba(X_test)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(rf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our thing \n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 1095,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit our thing\n",
    "knn.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the prediction\n",
    "y_pred = knn.predict(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estmating the probability\n",
    "y_pred_proba = knn.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.76\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.89      0.79      3484\n",
      "           1       0.85      0.63      0.72      3484\n",
      "\n",
      "    accuracy                           0.76      6968\n",
      "   macro avg       0.78      0.76      0.76      6968\n",
      "weighted avg       0.78      0.76      0.76      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: solver = lbfgs, c = 1\n",
      "Accuracy: 0.55\n",
      "[[2531 1932]\n",
      " [ 454  417]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.57      0.68      4463\n",
      "           1       0.18      0.48      0.26       871\n",
      "\n",
      "    accuracy                           0.55      5334\n",
      "   macro avg       0.51      0.52      0.47      5334\n",
      "weighted avg       0.74      0.55      0.61      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "y_pred_proba = knn.predict_proba(X_test)\n",
    "\n",
    "print(\"Model 1: solver = lbfgs, c = 1\")\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(knn.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create out thing\n",
    "logit = LogisticRegression(C=1, random_state=19, intercept_scaling=1, solver='liblinear', class_weight = 'balanced',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight='balanced', random_state=19,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the thing\n",
    "logit.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the predicition\n",
    "y_pred = logit.predict(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate the probability\n",
    "y_pred_proba = logit.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.54\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1922 1562]\n",
      " [1633 1851]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.55      0.55      3484\n",
      "           1       0.54      0.53      0.54      3484\n",
      "\n",
      "    accuracy                           0.54      6968\n",
      "   macro avg       0.54      0.54      0.54      6968\n",
      "weighted avg       0.54      0.54      0.54      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n",
      "[[2497 1966]\n",
      " [ 435  436]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.56      0.68      4463\n",
      "           1       0.18      0.50      0.27       871\n",
      "\n",
      "    accuracy                           0.55      5334\n",
      "   macro avg       0.52      0.53      0.47      5334\n",
      "weighted avg       0.74      0.55      0.61      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = logit.predict(X_test)\n",
    "y_pred_proba = logit.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(logit.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Booster Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the object\n",
    "clf = GradientBoostingClassifier(max_depth=3, random_state=19, n_estimators = 100, min_samples_leaf=6,learning_rate = 0.05).fit(X_resampled, y_resampled)\n",
    "# fitting our thing\n",
    "clf = clf.fit(X_resampled, y_resampled)\n",
    "#creating the prediction\n",
    "y_pred = clf.predict(X_resampled)\n",
    "#creating prediction probabaility\n",
    "y_pred_proba = clf.predict_proba(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.70\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_resampled, y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2516</td>\n",
       "      <td>968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1128</td>\n",
       "      <td>2356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  2516   968\n",
       "1  1128  2356"
      ]
     },
     "execution_count": 1113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(y_resampled.unique())\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y_resampled, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71      3484\n",
      "           1       0.71      0.68      0.69      3484\n",
      "\n",
      "    accuracy                           0.70      6968\n",
      "   macro avg       0.70      0.70      0.70      6968\n",
      "weighted avg       0.70      0.70      0.70      6968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_resampled, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test Eval #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "[[3131 1332]\n",
      " [ 298  573]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.70      0.79      4463\n",
      "           1       0.30      0.66      0.41       871\n",
      "\n",
      "    accuracy                           0.69      5334\n",
      "   macro avg       0.61      0.68      0.60      5334\n",
      "weighted avg       0.81      0.69      0.73      5334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
